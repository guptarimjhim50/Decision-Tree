{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPDLpdrZ8yHDU9Aiz2E3e34"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Question 1: What is a Decision Tree, and how does it work in the context of classification?\n","\n","Answer:\n","\n","A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. It models decisions and their possible consequences in a tree-like structure, where each internal node represents a test on an attribute (feature), each branch corresponds to the outcome of the test, and each leaf node represents a class label (decision).\n","\n","In the context of classification, the decision tree works as follows:\n","\n","Root Node: The tree starts with a root node that represents the entire dataset.\n","\n","Splitting: At each node, the algorithm selects the best feature and threshold to split the data into subsets that are more \"pure\" (i.e., containing instances mostly from one class).\n","\n","Recursive Partitioning: This splitting process is applied recursively to each child node until one of the stopping criteria is met, such as reaching a maximum depth, having all samples belong to one class, or having too few samples to split.\n","\n","Leaf Nodes: The terminal nodes, or leaves, assign the most common class among the samples in that node as the predicted class.\n","\n","By repeatedly splitting the data based on features that best separate the classes, the decision tree learns a set of rules that can classify new, unseen data points by traversing the tree from root to leaf based on their feature values.\n","\n","Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n","\n","Answer:\n","\n","Gini Impurity and Entropy are two common impurity measures used to evaluate how well a decision tree split separates the data into classes.\n","\n","Gini Impurity:\n","It measures the probability of incorrectly classifying a randomly chosen element if it were randomly labeled according to the distribution of classes in the node.\n","Mathematically, for a node with classes\n","ğ‘\n","1\n",",\n","ğ‘\n","2\n",",\n",".\n",".\n",".\n",",\n","ğ‘\n","ğ‘˜\n","c\n","1\n","\tâ€‹\n","\n",",c\n","2\n","\tâ€‹\n","\n",",...,c\n","k\n","\tâ€‹\n","\n",",\n","\n","ğº\n","ğ‘–\n","ğ‘›\n","ğ‘–\n","=\n","1\n","âˆ’\n","âˆ‘\n","ğ‘–\n","=\n","1\n","ğ‘˜\n","ğ‘\n","ğ‘–\n","2\n","Gini=1âˆ’\n","i=1\n","âˆ‘\n","k\n","\tâ€‹\n","\n","p\n","i\n","2\n","\tâ€‹\n","\n","\n","where\n","ğ‘\n","ğ‘–\n","p\n","i\n","\tâ€‹\n","\n"," is the proportion of class\n","ğ‘–\n","i in the node.\n","A Gini impurity of 0 indicates a perfectly pure node (only one class).\n","\n","Entropy:\n","It measures the amount of disorder or uncertainty in the node.\n","It is defined as:\n","\n","ğ¸\n","ğ‘›\n","ğ‘¡\n","ğ‘Ÿ\n","ğ‘œ\n","ğ‘\n","ğ‘¦\n","=\n","âˆ’\n","âˆ‘\n","ğ‘–\n","=\n","1\n","ğ‘˜\n","ğ‘\n","ğ‘–\n","log\n","â¡\n","2\n","ğ‘\n","ğ‘–\n","Entropy=âˆ’\n","i=1\n","âˆ‘\n","k\n","\tâ€‹\n","\n","p\n","i\n","\tâ€‹\n","\n","log\n","2\n","\tâ€‹\n","\n","p\n","i\n","\tâ€‹\n","\n","\n"," is the probability of class\n","ğ‘–\n","i. Entropy is 0 for a pure node and maximal when the classes are equally distributed.\n","\n","Impact on Splits:\n","\n","Both measures are used to evaluate candidate splits during tree construction.\n","\n","The algorithm calculates the weighted impurity of child nodes after a split and chooses the split that results in the largest reduction in impurity (called information gain in the case of entropy, and Gini gain or Gini decrease for Gini impurity).\n","\n","This means the split that best separates the classes (creating purer child nodes) is preferred.\n","\n","Though they differ mathematically, both generally lead to similar splits in practice. However, Gini impurity tends to be faster to compute and is often preferred in implementations like CART (Classification and Regression Trees), while entropy is rooted in information theory."],"metadata":{"id":"McMMKuTAI8Bo"}},{"cell_type":"markdown","source":["Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n","\n","Answer:\n","\n","Pre-Pruning and Post-Pruning are two techniques used to prevent overfitting in decision trees by controlling the tree's complexity.\n","\n","Pre-Pruning (Early Stopping):\n","Pre-pruning stops the tree from growing beyond a certain point during the training process. The tree stops splitting nodes if a certain condition is met, such as:\n","\n","The number of samples in a node is below a threshold.\n","\n","The maximum depth of the tree is reached.\n","\n","The improvement in impurity is too small to justify further splitting.\n","This way, the tree grows only as large as necessary, avoiding unnecessary splits.\n","\n","Practical advantage:\n","\n","Faster training time because the tree stops growing early, reducing computational cost and making it efficient for large datasets.\n","\n","Post-Pruning (Pruning after full growth):\n","Post-pruning involves first growing the full tree to its maximum depth and then trimming back some branches that do not provide significant predictive power. This is usually done by evaluating the treeâ€™s performance on a validation set and removing branches that reduce overfitting.\n","\n","Practical advantage:\n","\n","Better accuracy because the model is fully grown first, capturing all potential patterns, and then simplified based on validation, which can lead to more robust generalization.\n","\n","Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n","\n","Answer:\n","\n","Information Gain is a metric used in decision trees to measure how much a feature split improves the purity of the resulting subsets compared to the original dataset.\n","\n","It is calculated as the difference between the impurity (usually entropy) of the parent node and the weighted sum of the impurities of the child nodes after the split.\n","Mathematically:\n","\n","InformationÂ Gain\n","=\n","ğ¸\n","ğ‘›\n","ğ‘¡\n","ğ‘Ÿ\n","ğ‘œ\n","ğ‘\n","ğ‘¦\n","(\n","ğ‘\n","ğ‘\n","ğ‘Ÿ\n","ğ‘’\n","ğ‘›\n","ğ‘¡\n",")\n","âˆ’\n","âˆ‘\n","ğ‘–\n","ğ‘\n","ğ‘–\n","ğ‘\n","ğ¸\n","ğ‘›\n","ğ‘¡\n","ğ‘Ÿ\n","ğ‘œ\n","ğ‘\n","ğ‘¦\n","(\n","ğ‘\n","â„\n","ğ‘–\n","ğ‘™\n","ğ‘‘\n","ğ‘–\n",")\n","InformationÂ Gain=Entropy(parent)âˆ’\n","i\n","âˆ‘\n","\tâ€‹\n","\n","N\n","N\n","i\n","\tâ€‹\n","\n","\tâ€‹\n","\n","Entropy(child\n","i\n","\tâ€‹\n","\n",")\n","\n","where\n","ğ‘\n","ğ‘–\n","N\n","i\n","\tâ€‹\n","\n"," is the number of samples in child\n","ğ‘–\n","i, and\n","ğ‘\n","N is the total number of samples in the parent node.\n","\n","The idea is that a good split will reduce the uncertainty or disorder of the classes in the child nodes, making them more homogeneous.\n","\n","Why is Information Gain important?\n","\n","It guides the decision tree algorithm in choosing the best feature and threshold to split on at each step.\n","\n","By maximizing information gain, the tree efficiently partitions the data into subsets that are easier to classify, leading to better predictive performance."],"metadata":{"id":"Td0fTS10JPP-"}},{"cell_type":"markdown","source":["Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n","\n","Answer:\n","\n","Common Real-World Applications of Decision Trees:\n","\n","Medical Diagnosis:\n","Decision trees help classify patients based on symptoms, medical history, and test results to diagnose diseases or predict health risks.\n","\n","Credit Scoring and Loan Approval:\n","Financial institutions use decision trees to evaluate creditworthiness by analyzing applicantsâ€™ financial data, payment history, and demographics.\n","\n","Customer Segmentation and Marketing:\n","Businesses segment customers based on buying behavior, preferences, and demographics to tailor marketing campaigns and improve customer targeting.\n","\n","Fraud Detection:\n","Decision trees identify suspicious transactions by learning patterns from past fraudulent and legitimate activities.\n","\n","Manufacturing and Quality Control:\n","Used to predict product defects and optimize production processes by analyzing operational parameters.\n","\n","Churn Prediction:\n","Telecom and subscription services use decision trees to predict customers likely to cancel services, enabling proactive retention strategies.\n","\n","Main Advantages of Decision Trees:\n","\n","Easy to Understand and Interpret:\n","The tree structure mimics human decision-making, making results transparent and explainable.\n","\n","No Need for Data Normalization:\n","Decision trees can handle both numerical and categorical data without preprocessing like scaling.\n","\n","Handles Non-linear Relationships:\n","They capture complex interactions between features naturally.\n","\n","Fast Prediction:\n","Once trained, decision trees are computationally efficient in making predictions.\n","\n","Main Limitations of Decision Trees:\n","\n","Prone to Overfitting:\n","Trees can become too complex and fit noise in the training data, reducing generalization on unseen data.\n","\n","Instability:\n","Small changes in data can lead to very different tree structures, causing high variance.\n","\n","Biased with Imbalanced Data:\n","Decision trees may favor classes that dominate the training set unless balanced properly.\n","\n","Limited Predictive Accuracy:\n","Single decision trees often perform worse than ensemble methods like Random Forests or Gradient Boosted Trees."],"metadata":{"id":"4LsizAnbJTqB"}},{"cell_type":"markdown","source":["Question 6:\n","\n","Write a Python program to:\n","\n","Load the Iris Dataset\n","\n","Train a Decision Tree Classifier using the Gini criterion\n","\n","Print the modelâ€™s accuracy and feature importances"],"metadata":{"id":"ZZb5BHJ4JUwC"}},{"cell_type":"code","source":["from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load Iris dataset\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","# Split dataset into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Train Decision Tree Classifier with Gini criterion\n","clf = DecisionTreeClassifier(criter\n","ion='gini', random_state=42)\n","clf.fit(X_train, y_train)\n","\n","# Predict on test data\n","y_pred = clf.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# Print accuracy and feature importances\n","print(f\"Accuracy of Decision Tree Classifier (Gini): {accuracy:.4f}\")\n","print(\"Feature Importances:\")\n","for name, importance in zip(iris.feature_names, clf.feature_importances_):\n","    print(f\"{name}: {importance:.4f}\")\n"],"metadata":{"id":"He1J0hYOJ0ne"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Accuracy of Decision Tree Classifier (Gini): 0.9556\n","Feature Importances:\n","sepal length (cm): 0.0000\n","sepal width (cm): 0.0140\n","petal length (cm): 0.4690\n","petal width (cm): 0.5170\n"],"metadata":{"id":"8AHfrrUQJ6VQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Question 7:\n","\n","Write a Python program to:\n","\n","Load the Iris Dataset\n","\n","Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree"],"metadata":{"id":"xETL2MxxJ8pv"}},{"cell_type":"code","source":["from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load Iris dataset\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","# Split dataset into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Train fully-grown Decision Tree (no max_depth)\n","clf_full = DecisionTreeClassifier(random_state=42)\n","clf_full.fit(X_train, y_train)\n","y_pred_full = clf_full.predict(X_test)\n","accuracy_full = accuracy_score(y_test, y_pred_full)\n","\n","# Train Decision Tree with max_depth=3\n","clf_depth3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n","clf_depth3.fit(X_train, y_train)\n","y_pred_depth3 = clf_depth3.predict(X_test)\n","accuracy_depth3 = accuracy_score(y_test, y_pred_depth3)\n","\n","# Print accuracies\n","print(f\"Accuracy of fully-grown Decision Tree: {accuracy_full:.4f}\")\n","print(f\"Accuracy of Decision Tree with max_depth=3: {accuracy_depth3:.4f}\")\n"],"metadata":{"id":"cp2jwbD_KB4V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Accuracy of fully-grown Decision Tree: 0.9556\n","Accuracy of Decision Tree with max_depth=3: 0.9556\n"],"metadata":{"id":"AoLrd1GyKJL1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Question 8:\n","\n","Write a Python program to:\n","\n","Load the Boston Housing Dataset\n","\n","Train a Decision Tree Regressor\n","\n","Print the Mean Squared Error (MSE) and feature importances"],"metadata":{"id":"wvM2feelKKhV"}},{"cell_type":"code","source":["from sklearn.datasets import load_boston\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.metrics import mean_squared_error\n","\n","# Load Boston Housing dataset\n","boston = load_boston()\n","X, y = boston.data, boston.target\n","\n","# Split dataset into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Train Decision Tree Regressor\n","regressor = DecisionTreeRegressor(random_state=42)\n","regressor.fit(X_train, y_train)\n","\n","# Predict on test data\n","y_pred = regressor.predict(X_test)\n","\n","# Calculate Mean Squared Error (MSE)\n","mse = mean_squared_error(y_test, y_pred)\n","\n","# Print MSE and feature importances\n","print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n","print(\"Feature Importances:\")\n","for name, importance in zip(boston.feature_names, regressor.feature_importances_):\n","    print(f\"{name}: {importance:.4f}\")\n"],"metadata":{"id":"jJXgMFjlKS0r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Mean Squared Error (MSE): 18.6753\n","Feature Importances:\n","CRIM: 0.0611\n","ZN: 0.0000\n","INDUS: 0.0051\n","CHAS: 0.0015\n","NOX: 0.0509\n","RM: 0.5514\n","AGE: 0.0093\n","DIS: 0.0762\n","RAD: 0.0111\n","TAX: 0.0272\n","PTRATIO: 0.0895\n","B: 0.0237\n","LSTAT: 0.0939\n"],"metadata":{"id":"pQb_YSAcKYSu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Question 9:\n","\n","Write a Python program to:\n","\n","Load the Iris Dataset\n","\n","Tune the Decision Treeâ€™s max_depth and min_samples_split using GridSearchCV\n","\n","Print the best parameters and the resulting model accuracy"],"metadata":{"id":"RpTxGtXjKVPc"}},{"cell_type":"code","source":["from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load Iris dataset\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","# Split dataset into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Define parameter grid for tuning\n","param_grid = {\n","    'max_depth': [2, 3, 4, 5, None],\n","    'min_samples_split': [2, 5, 10]\n","}\n","\n","# Initialize Decision Tree Classifier\n","clf = DecisionTreeClassifier(random_state=42)\n","\n","# Initialize GridSearchCV\n","grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy')\n","\n","# Fit GridSearch to training data\n","grid_search.fit(X_train, y_train)\n","\n","# Best parameters and best estimator\n","best_params = grid_search.best_params_\n","best_estimator = grid_search.best_estimator_\n","\n","# Predict on test data with best estimator\n","y_pred = best_estimator.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# Print results\n","print(\"Best parameters found by GridSearchCV:\")\n","print(best_params)\n","print(f\"Accuracy with best parameters: {accuracy:.4f}\")\n"],"metadata":{"id":"wh6G1sSSKesn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Best parameters found by GridSearchCV:\n","{'max_depth': 3, 'min_samples_split': 2}\n","Accuracy with best parameters: 0.9556\n"],"metadata":{"id":"zXhgpITEKiU7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Question 10: Step-by-step process to build a Decision Tree model for disease prediction with mixed data and missing values"],"metadata":{"id":"6ki3nhKUKk5k"}},{"cell_type":"markdown","source":["1. Handle the Missing Values:\n","\n","Identify missing data:\n","Check which features contain missing values and the percentage missing.\n","\n","Decide on imputation strategy:\n","\n","For numerical features, use mean, median, or more advanced methods like K-Nearest Neighbors (KNN) imputation.\n","\n","For categorical features, replace missing values with the most frequent category or create a new category labeled â€œmissing.â€\n","\n","Apply imputation:\n","Use libraries like sklearn.impute.SimpleImputer or IterativeImputer to fill missing values consistently on both training and test sets.\n","\n","2. Encode the Categorical Features:\n","\n","Convert categorical variables into numerical format for the Decision Tree model to process.\n","\n","If categories are ordinal (have order), use Label Encoding.\n","\n","If categories are nominal (no order), use One-Hot Encoding to create binary columns for each category.\n","\n","Use pandas.get_dummies() or sklearn.preprocessing.OneHotEncoder for this step.\n","\n","3. Train a Decision Tree Model:\n","\n","Split the data into training and testing sets to evaluate generalization.\n","\n","Initialize a DecisionTreeClassifier, optionally setting a random state for reproducibility.\n","\n","Train the model on the processed training data.\n","\n","4. Tune Hyperparameters:\n","\n","Use techniques like GridSearchCV or RandomizedSearchCV to optimize key hyperparameters such as:\n","\n","max_depth (tree depth)\n","\n","min_samples_split (minimum samples to split a node)\n","\n","min_samples_leaf (minimum samples at a leaf node)\n","\n","criterion (e.g., \"gini\" or \"entropy\")\n","\n","Perform cross-validation to avoid overfitting and select the best combination of parameters.\n","\n","5. Evaluate Model Performance:\n","\n","Use appropriate metrics for classification:\n","\n","Accuracy (overall correctness)\n","\n","Precision and Recall (important for imbalanced datasets)\n","\n","F1-score (harmonic mean of precision and recall)\n","\n","ROC-AUC (for measuring ability to discriminate classes)\n","\n","Generate a confusion matrix to understand types of errors.\n","\n","Test the model on the test set to estimate real-world performance.\n","\n","Business Value of the Model:\n","\n","Early disease detection:\n","The model helps identify patients at high risk early, enabling timely intervention and treatment.\n","\n","Resource optimization:\n","Healthcare providers can prioritize patients who need immediate attention, improving resource allocation.\n","\n","Cost reduction:\n","Preventing advanced disease stages reduces treatment costs and hospitalization rates.\n","\n","Personalized healthcare:\n","Insights from the model can support tailored treatment plans based on patient risk profiles.\n","\n","Improved patient outcomes:\n","Early and accurate predictions contribute to better prognosis and quality of life."],"metadata":{"id":"ZGVfAJA2KsyW"}}]}